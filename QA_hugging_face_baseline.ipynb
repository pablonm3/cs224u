{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "IPython.notebook.set_autosave_interval(10000)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autosaving every 10 seconds\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline, AutoModel, AutoTokenizer, BertTokenizer, BertForQuestionAnswering\n",
    "import transformers\n",
    "import torch\n",
    "import pandas as pd\n",
    "%autosave 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "DATA_DIRECTORY = \"/home/xcs224u_student/notebooks/cs224u/data\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# baselines models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT model trained on squad 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This model is BERT base uncased trained on SQuAD v2 \n",
    "# https://huggingface.co/twmkn9/bert-base-uncased-squad2\n",
    "BERT_MODEL = \"twmkn9/bert-base-uncased-squad2\"\n",
    "qa_bert_pipeline = pipeline('question-answering', model=BERT_MODEL, tokenizer=BERT_MODEL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 0.9665267544103031, 'start': 0, 'end': 6, 'answer': 'Normans'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa_bert_pipeline(context=\"Normans\", question=\"Who gave their name to Normandy in the 1000's and 1100's\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 0.9665267544103031, 'start': 0, 'end': 6, 'answer': 'Normans'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa_bert_pipeline(context=\"Normans\", question=\"Who gave their name to Normandy in the 1000's and 1100's\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline model using SciBERT - pretrained with SQUAD V2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# allenai/scibert_scivocab_uncased: scibert model fine-tuned on SQuAD V2 \n",
    "# https://huggingface.co/ktrapeznikov/scibert_scivocab_uncased_squad_v2\n",
    "SCIBERT_MODEL = \"ktrapeznikov/scibert_scivocab_uncased_squad_v2\"\n",
    "qa_scibert_pipeline = pipeline('question-answering', model=SCIBERT_MODEL, tokenizer=SCIBERT_MODEL)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline model using BIOBERT - pretrained with SQUAD V2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# biobert model fine-tuned on SQuAD V2 \n",
    "# https://huggingface.co/ktrapeznikov/biobert_v1.1_pubmed_squad_v2\n",
    "BIOBERT_MODEL = \"ktrapeznikov/biobert_v1.1_pubmed_squad_v2\"\n",
    "qa_biobert_pipeline = pipeline('question-answering', model=BIOBERT_MODEL, tokenizer=BIOBERT_MODEL)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Qualitative comparisons on covid-19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONVID_ARTICLE_EASY=\"\"\"Coronavirus disease 2019 (COVID-19) is an infectious disease caused by severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2).\n",
    "The disease was first identified in December 2019 in Wuhan, the capital of China's Hubei province, and has since spread globally, resulting in the ongoing 2019–20 coronavirus pandemic.\n",
    "Common symptoms include fever, cough and shortness of breath.[5] Other symptoms may include fatigue, muscle pain, diarrhoea, sore throat, loss of smell and abdominal pain.[5][11][12] The time from exposure to onset of symptoms is typically around five days, but may range from two to fourteen days.[5][13] While the majority of cases result in mild symptoms, some progress to viral pneumonia and multi-organ failure.\n",
    "As of 10 April 2020, more than 1.67 million[7] cases have been reported in more than 200 countries and territories,[15] resulting in more than 101,000 deaths.[7] More than 372,000 people have recovered. The virus is mainly spread between people during close contact,[a] often via small droplets produced during coughing,[b] sneezing, or talking.\n",
    "While these droplets are produced when breathing out,\n",
    "they usually fall to the ground or surfaces rather than being infectious over large distances.[6][19][20] People may also become infected by touching a contaminated surface and then their face.\n",
    "The virus can survive on surfaces for up to 72 hours.\n",
    "Coronavirus is most contagious during the first three days after onset of symptoms, although spread may be possible before symptoms\n",
    "appear and in later stages of the disease.The standard method of diagnosis is by real-time reverse transcription polymerase chain reaction (rRT-PCR) from a nasopharyngeal swab.[23] The infection can also be diagnosed from a combination of symptoms, risk factors and a chest CT scan showing features of pneumonia.Recommended measures to prevent infection include frequent hand washing, maintaining physical distance from others (especially from those with symptoms), covering coughs and sneezes with a tissue or inner elbow and keeping unwashed hands away from the face.\n",
    "The use of masks is recommended for those who suspect they have the virus and their caregivers.[28] Recommendations for mask use by the general public vary, with some authorities recommending against their use, some recommending their use and others requiring their use.\n",
    "Currently, there is no vaccine or specific antiviral treatment for COVID-19.\n",
    "Management involves treatment of symptoms, supportive care, isolation and experimental measures.The World Health Organization (WHO) declared the 2019–20 coronavirus outbreak a Public Health Emergency of International Concern (PHEIC)[33][34] on 30 January 2020 and a pandemic on 11 March 2020.[10] Local transmission of the disease has been recorded in many countries across all six WHO regions.[35]\"\"\"\n",
    "CONVID_ARTICLE_MEDIUM1=\"\"\"A familial cluster of 5 patients with COVID-19\n",
    "pneumonia in Anyang, China, had contact before their symptom onset with an asymptomatic family member who had\n",
    "traveled from the epidemic center of Wuhan. The sequence\n",
    "of events suggests that the coronavirus may have been transmitted by the asymptomatic carrier. The incubation period\n",
    "for patient 1 was 19 days, which is long but within the\n",
    "reported range of 0 to 24 days.4 Her first RT-PCR result was\n",
    "negative; false-negative results have been observed related to\n",
    "the quality of the kit, the collected sample, or performance of\n",
    "the test. RT-PCR has been widely deployed in diagnostic\n",
    "virology and has yielded few false-positive outcomes.5 Thus,\n",
    "her second RT-PCR result was unlikely to have been a falsepositive and was used to define infection with the coronavirus that causes COVID-19.\n",
    "One previous study reported an asymptomatic 10-yearold boy with COVID-19 infection, but he had abnormalities\n",
    "on chest CT.6 If the findings in this report of presumed transmission by an asymptomatic carrier are replicated, the prevention of COVID-19 infection would prove challenging.\n",
    "The mechanism by which asymptomatic carriers could acquire and transmit the coronavirus that causes COVID-19\n",
    "requires further study\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT SQUAD2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 0.5013612845677784,\n",
       " 'start': 342,\n",
       " 'end': 382,\n",
       " 'answer': 'fever, cough and shortness of breath.[5]'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa_bert_pipeline(context=CONVID_ARTICLE_EASY, question='What are the symptoms')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 0.09125602338691152,\n",
       " 'start': 644,\n",
       " 'end': 685,\n",
       " 'answer': 'has yielded few false-positive outcomes.5'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa_bert_pipeline(context=CONVID_ARTICLE_MEDIUM1, question='does RT-PCR work')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SCIBERT SQUAD2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 0.19227549805068556,\n",
       " 'start': 318,\n",
       " 'end': 382,\n",
       " 'answer': 'Common symptoms include fever, cough and shortness of breath.[5]'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa_scibert_pipeline(context=CONVID_ARTICLE_EASY, question='What are the symptoms')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 0.1103179816038562,\n",
       " 'start': 644,\n",
       " 'end': 685,\n",
       " 'answer': 'has yielded few false-positive outcomes.5'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa_scibert_pipeline(context=CONVID_ARTICLE_MEDIUM1, question='does RT-PCR work')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BIOBERT SQUAD2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 0.4340334721582906,\n",
       " 'start': 342,\n",
       " 'end': 382,\n",
       " 'answer': 'fever, cough and shortness of breath.[5]'}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa_biobert_pipeline(context=CONVID_ARTICLE_EASY, question='What are the symptoms')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 0.09305679627574825,\n",
       " 'start': 585,\n",
       " 'end': 639,\n",
       " 'answer': 'RT-PCR has been widely deployed in diagnostic virology'}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa_biobert_pipeline(context=CONVID_ARTICLE_MEDIUM1, question='does RT-PCR work')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# baseline metrics on SQUAD2:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT base uncased SQUAD2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### command:\n",
    "\n",
    "\n",
    "python run_squad.py \\\n",
    "  --model_type bert \\\n",
    "  --model_name_or_path twmkn9/bert-base-uncased-squad2 \\\n",
    "  --version_2_with_negative \\\n",
    "  --do_lower_case \\\n",
    "  --do_eval \\\n",
    "  --predict_file  /data/home/xcs224u_student/notebooks/cs224u/data/squad/dev-v2.0.json\\\n",
    "  --per_gpu_eval_batch_size 12 \\\n",
    "  --max_answer_length 128 \\\n",
    "  --max_seq_length 384 \\\n",
    "  --doc_stride 128 \\\n",
    "  --overwrite_cache \\\n",
    "  --output_dir /data/home/xcs224u_student/notebooks/cs224u/data/squad/pred_outputs/bert"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "{'exact': 72.21426766613324, 'f1': 75.74594024678946, 'total': 11873, 'HasAns_exact': 72.55398110661268, 'HasAns_f1': 79.62745420886144, 'HasAns_total': 5928, 'NoAns_exact': 71.87552565180825, 'NoAns_f1': 71.87552565180825, 'NoAns_total': 5945, 'best_exact': 72.21426766613324, 'best_exact_thresh': 0.0, 'best_f1': 75.74594024678946, 'best_f1_thresh': 0.0}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## biobert SQUAD2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### command: \n",
    "\n",
    "python run_squad.py \\\n",
    "  --model_type bert \\\n",
    "  --model_name_or_path ktrapeznikov/biobert_v1.1_pubmed_squad_v2 \\\n",
    "  --version_2_with_negative \\\n",
    "  --do_eval \\\n",
    "  --predict_file  /data/home/xcs224u_student/notebooks/cs224u/data/squad/dev-v2.0.json\\\n",
    "  --per_gpu_eval_batch_size 12 \\\n",
    "  --max_answer_length 128 \\\n",
    "  --max_seq_length 384 \\\n",
    "  --doc_stride 128 \\\n",
    "  --overwrite_cache \\\n",
    "  --output_dir /data/home/xcs224u_student/notebooks/cs224u/data/squad/pred_outputs/biobert\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'exact': 75.92857744462225,\n",
       " 'f1': 79.33147378635734,\n",
       " 'total': 11873,\n",
       " 'HasAns_exact': 74.07219973009447,\n",
       " 'HasAns_f1': 80.88775105692005,\n",
       " 'HasAns_total': 5928,\n",
       " 'NoAns_exact': 77.77964676198486,\n",
       " 'NoAns_f1': 77.77964676198486,\n",
       " 'NoAns_total': 5945,\n",
       " 'best_exact': 75.92857744462225,\n",
       " 'best_exact_thresh': 0.0,\n",
       " 'best_f1': 79.33147378635738,\n",
       " 'best_f1_thresh': 0.0}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{'exact': 75.92857744462225, 'f1': 79.33147378635734, 'total': 11873, 'HasAns_exact': 74.07219973009447, 'HasAns_f1': 80.88775105692005, 'HasAns_total': 5928, 'NoAns_exact': 77.77964676198486, 'NoAns_f1': 77.77964676198486, 'NoAns_total': 5945, 'best_exact': 75.92857744462225, 'best_exact_thresh': 0.0, 'best_f1': 79.33147378635738, 'best_f1_thresh': 0.0}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## scibert SQUAD2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### command:\n",
    "python run_squad.py \\\n",
    "  --model_type bert \\\n",
    "  --model_name_or_path ktrapeznikov/scibert_scivocab_uncased_squad_v2 \\\n",
    "  --version_2_with_negative \\\n",
    "  --do_lower_case \\\n",
    "  --do_eval \\\n",
    "  --predict_file  /data/home/xcs224u_student/notebooks/cs224u/data/squad/dev-v2.0.json\\\n",
    "  --per_gpu_eval_batch_size 12 \\\n",
    "  --max_answer_length 128 \\\n",
    "  --max_seq_length 384 \\\n",
    "  --doc_stride 128 \\\n",
    "  --overwrite_cache \\\n",
    "  --output_dir /data/home/xcs224u_student/notebooks/cs224u/data/squad/pred_outputs/scibert\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'exact': 74.98526067548218,\n",
       " 'f1': 78.44016223271369,\n",
       " 'total': 11873,\n",
       " 'HasAns_exact': 70.58029689608637,\n",
       " 'HasAns_f1': 77.5000077916683,\n",
       " 'HasAns_total': 5928,\n",
       " 'NoAns_exact': 79.37762825904122,\n",
       " 'NoAns_f1': 79.37762825904122,\n",
       " 'NoAns_total': 5945,\n",
       " 'best_exact': 74.99368314663522,\n",
       " 'best_exact_thresh': 0.0,\n",
       " 'best_f1': 78.44858470386681,\n",
       " 'best_f1_thresh': 0.0}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{'exact': 74.98526067548218, 'f1': 78.44016223271369, 'total': 11873, 'HasAns_exact': 70.58029689608637, 'HasAns_f1': 77.5000077916683, 'HasAns_total': 5928, 'NoAns_exact': 79.37762825904122, 'NoAns_f1': 79.37762825904122, 'NoAns_total': 5945, 'best_exact': 74.99368314663522, 'best_exact_thresh': 0.0, 'best_f1': 78.44858470386681, 'best_f1_thresh': 0.0}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# baseline metrics on BIOASQ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT base uncased SQUAD2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### command: \n",
    "\n",
    "````\n",
    "export TEST_SET_NAME=BioASQ-task8bPhaseB-testset_combined_squad_format.json\n",
    "export DATA_DIRECTORY=/data/home/xcs224u_student/notebooks/cs224u/data/bioasq\n",
    "python run_squad.py \\\n",
    "  --model_type bert \\\n",
    "  --model_name_or_path twmkn9/bert-base-uncased-squad2 \\\n",
    "  --version_2_with_negative \\\n",
    "  --do_lower_case \\\n",
    "  --do_eval \\\n",
    "  --predict_file $DATA_DIRECTORY/BioASQ-test8b/$TEST_SET_NAME\\\n",
    "  --per_gpu_eval_batch_size 12 \\\n",
    "  --max_answer_length 128 \\\n",
    "  --max_seq_length 384 \\\n",
    "  --doc_stride 128 \\\n",
    "  --overwrite_cache \\\n",
    "  --output_dir $DATA_DIRECTORY/pred_outputs/bert \n",
    "  ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "{'exact': 0.7230255839822024, 'f1': 16.179300126709233, 'total': 1798, 'HasAns_exact': 0.7230255839822024, 'HasAns_f1': 16.179300126709233, 'HasAns_total': 1798, 'best_exact': 0.7230255839822024, 'best_exact_thresh': 0.0, 'best_f1': 16.179300126709233, 'best_f1_thresh': 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## biobert SQUAD2(this model is cased, so don't lowercase)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "export TEST_SET_NAME=BioASQ-task8bPhaseB-testset_combined_squad_format.json\n",
    "export DATA_DIRECTORY=/data/home/xcs224u_student/notebooks/cs224u/data/bioasq\n",
    "python run_squad.py \\\n",
    "  --model_type bert \\\n",
    "  --model_name_or_path ktrapeznikov/biobert_v1.1_pubmed_squad_v2 \\\n",
    "  --version_2_with_negative \\\n",
    "  --do_eval \\\n",
    "  --predict_file $DATA_DIRECTORY/BioASQ-test8b/$TEST_SET_NAME\\\n",
    "  --per_gpu_eval_batch_size 12 \\\n",
    "  --max_answer_length 128 \\\n",
    "  --max_seq_length 384 \\\n",
    "  --doc_stride 128 \\\n",
    "  --overwrite_cache \\\n",
    "  --output_dir $DATA_DIRECTORY/pred_outputs/biobert "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### results:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "{'exact': 0.8342602892102335, 'f1': 16.69560652289738, 'total': 1798, 'HasAns_exact': 0.8342602892102335, 'HasAns_f1': 16.69560652289738, 'HasAns_total': 1798, 'best_exact': 0.8342602892102335, 'best_exact_thresh': 0.0, 'best_f1': 16.69560652289738, 'best_f1_thresh': 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## scibert finedtuned on squad2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "export TEST_SET_NAME=BioASQ-task8bPhaseB-testset_combined_squad_format.json\n",
    "export DATA_DIRECTORY=/data/home/xcs224u_student/notebooks/cs224u/data/bioasq\n",
    "python run_squad.py \\\n",
    "  --model_type bert \\\n",
    "  --model_name_or_path ktrapeznikov/scibert_scivocab_uncased_squad_v2 \\\n",
    "   --version_2_with_negative \\\n",
    "  --do_lower_case \\\n",
    "  --do_eval \\\n",
    "  --predict_file $DATA_DIRECTORY/BioASQ-test8b/$TEST_SET_NAME\\\n",
    "  --per_gpu_eval_batch_size 12 \\\n",
    "  --max_answer_length 128 \\\n",
    "  --max_seq_length 384 \\\n",
    "  --doc_stride 128 \\\n",
    "  --overwrite_cache \\\n",
    "  --output_dir $DATA_DIRECTORY/pred_outputs/scibert \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'exact': 1.0567296996662958,\n",
       " 'f1': 18.87351618082947,\n",
       " 'total': 1798,\n",
       " 'HasAns_exact': 1.0567296996662958,\n",
       " 'HasAns_f1': 18.87351618082947,\n",
       " 'HasAns_total': 1798,\n",
       " 'best_exact': 1.0567296996662958,\n",
       " 'best_exact_thresh': 0.0,\n",
       " 'best_f1': 18.87351618082947,\n",
       " 'best_f1_thresh': 0.0}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{'exact': 1.0567296996662958, 'f1': 18.87351618082947, 'total': 1798, 'HasAns_exact': 1.0567296996662958, 'HasAns_f1': 18.87351618082947, 'HasAns_total': 1798, 'best_exact': 1.0567296996662958, 'best_exact_thresh': 0.0, 'best_f1': 18.87351618082947, 'best_f1_thresh': 0.0}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finetuning baseline models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BIOBERT(remember its cased)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "DATA_DIR=/home/xcs224u_student/notebooks/cs224u/data/bioasq\n",
    "OUTPUT_MODEL=/data/home/xcs224u_student/notebooks/cs224u/models/biobert_bioasq_try2\n",
    "BASE_MODEL=ktrapeznikov/biobert_v1.1_pubmed_squad_v2\n",
    "python run_squad.py \\\n",
    "  --version_2_with_negative \\\n",
    "  --model_type bert \\\n",
    "  --model_name_or_path $BASE_MODEL \\\n",
    "  --output_dir $OUTPUT_MODEL \\\n",
    "  --do_eval \\\n",
    "  --train_file $DATA_DIR/BioASQ-training8b/training8b_squad_format_3243.json \\\n",
    "  --predict_file $DATA_DIR/BioASQ-test8b/BioASQ-task8bPhaseB-testset_combined_squad_format.json \\\n",
    "  --per_gpu_train_batch_size 12 \\\n",
    "  --per_gpu_eval_batch_size 12 \\\n",
    "  --learning_rate 3e-5 \\\n",
    "  --num_train_epochs 3.0 \\\n",
    "  --max_seq_length 384 \\\n",
    "  --gradient_accumulation_steps 1 \\\n",
    "  --doc_stride 128 \\\n",
    "  --max_answer_length 128 \\\n",
    "  --save_steps 2000 \\\n",
    "  --threads 24 \\\n",
    "  --warmup_steps 550 \\\n",
    "  --gradient_accumulation_steps 1 \\\n",
    "  --fp16 \\\n",
    "  --logging_steps 50 \\\n",
    "  --do_train\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### metrics evaluated on bioasq:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'exact': 33.42602892102336,\n",
       " 'f1': 57.66916736505339,\n",
       " 'total': 1798,\n",
       " 'HasAns_exact': 33.42602892102336,\n",
       " 'HasAns_f1': 57.66916736505339,\n",
       " 'HasAns_total': 1798,\n",
       " 'best_exact': 33.42602892102336,\n",
       " 'best_exact_thresh': 0.0,\n",
       " 'best_f1': 57.66916736505339,\n",
       " 'best_f1_thresh': 0.0}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{'exact': 33.42602892102336, 'f1': 57.66916736505339, 'total': 1798, 'HasAns_exact': 33.42602892102336, 'HasAns_f1': 57.66916736505339, 'HasAns_total': 1798, 'best_exact': 33.42602892102336, 'best_exact_thresh': 0.0, 'best_f1': 57.66916736505339, 'best_f1_thresh': 0.0}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SCIBERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "DATA_DIR=/home/xcs224u_student/notebooks/cs224u/data/bioasq\n",
    "OUTPUT_MODEL=/data/home/xcs224u_student/notebooks/cs224u/models/scibert_bioasq_try5\n",
    "BASE_MODEL=ktrapeznikov/scibert_scivocab_uncased_squad_v2\n",
    "python run_squad.py \\\n",
    "  --version_2_with_negative \\\n",
    "  --model_type bert \\\n",
    "  --model_name_or_path $BASE_MODEL \\\n",
    "  --output_dir $OUTPUT_MODEL \\\n",
    "  --do_eval \\\n",
    "  --do_lower_case \\\n",
    "  --train_file $DATA_DIR/BioASQ-training8b/training8b_squad_format_3243.json \\\n",
    "  --predict_file $DATA_DIR/BioASQ-test8b/BioASQ-task8bPhaseB-testset_combined_squad_format.json \\\n",
    "  --per_gpu_train_batch_size 12 \\\n",
    "  --per_gpu_eval_batch_size 12 \\\n",
    "  --learning_rate 3e-5 \\\n",
    "  --num_train_epochs 3.0 \\\n",
    "  --max_seq_length 384 \\\n",
    "  --gradient_accumulation_steps 1 \\\n",
    "  --max_answer_length 128 \\\n",
    "  --doc_stride 128 \\\n",
    "  --save_steps 2000 \\\n",
    "  --threads 24 \\\n",
    "  --warmup_steps 550 \\\n",
    "  --gradient_accumulation_steps 1 \\\n",
    "  --fp16 \\\n",
    "  --logging_steps 50 \\\n",
    "  --do_train\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### metrics evaluated on bioasq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'exact': 35.81757508342603,\n",
       " 'f1': 60.45343274509798,\n",
       " 'total': 1798,\n",
       " 'HasAns_exact': 35.81757508342603,\n",
       " 'HasAns_f1': 60.45343274509798,\n",
       " 'HasAns_total': 1798,\n",
       " 'best_exact': 35.81757508342603,\n",
       " 'best_exact_thresh': 0.0,\n",
       " 'best_f1': 60.45343274509798,\n",
       " 'best_f1_thresh': 0.0}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{'exact': 35.81757508342603, 'f1': 60.45343274509798, 'total': 1798, 'HasAns_exact': 35.81757508342603, 'HasAns_f1': 60.45343274509798, 'HasAns_total': 1798, 'best_exact': 35.81757508342603, 'best_exact_thresh': 0.0, 'best_f1': 60.45343274509798, 'best_f1_thresh': 0.0}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finetune scibert-covid-squad on bioasq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## smoke test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "SCIBERT_COVID_SQUAD_MODEL = \"/data/home/xcs224u_student/notebooks/cs224u/models/scibert_covid_squad\"\n",
    "qa_scibert_covid_squad_pipeline = pipeline('question-answering', model=SCIBERT_COVID_SQUAD_MODEL, tokenizer=SCIBERT_COVID_SQUAD_MODEL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 0.31775387244431386,\n",
       " 'start': 318,\n",
       " 'end': 382,\n",
       " 'answer': 'Common symptoms include fever, cough and shortness of breath.[5]'}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa_scibert_covid_squad_pipeline(context=CONVID_ARTICLE_EASY, question='What are the symptoms')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## fine tuning on bioasq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### command: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "DATA_DIR=/home/xcs224u_student/notebooks/cs224u/data/bioasq\n",
    "OUTPUT_MODEL=/data/home/xcs224u_student/notebooks/cs224u/models/scibert_covid_squad_bioasq\n",
    "BASE_MODEL=/data/home/xcs224u_student/notebooks/cs224u/models/scibert_covid_squad\n",
    "python run_squad.py \\\n",
    "  --version_2_with_negative \\\n",
    "  --model_type bert \\\n",
    "  --model_name_or_path $BASE_MODEL \\\n",
    "  --output_dir $OUTPUT_MODEL \\\n",
    "  --do_eval \\\n",
    "  --do_lower_case \\\n",
    "  --train_file $DATA_DIR/BioASQ-training8b/training8b_squad_format_3243.json \\\n",
    "  --predict_file $DATA_DIR/BioASQ-test8b/BioASQ-task8bPhaseB-testset_combined_squad_format.json \\\n",
    "  --per_gpu_train_batch_size 12 \\\n",
    "  --per_gpu_eval_batch_size 12 \\\n",
    "  --learning_rate 3e-5 \\\n",
    "  --num_train_epochs 3.0 \\\n",
    "  --max_seq_length 384 \\\n",
    "  --gradient_accumulation_steps 1 \\\n",
    "  --max_answer_length 128 \\\n",
    "  --doc_stride 128 \\\n",
    "  --save_steps 2000 \\\n",
    "  --threads 24 \\\n",
    "  --warmup_steps 550 \\\n",
    "  --gradient_accumulation_steps 1 \\\n",
    "  --fp16 \\\n",
    "  --logging_steps 50 \\\n",
    "  --do_train\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## evaluation on bioasq:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'exact': 32.758620689655174,\n",
       " 'f1': 56.99536137666129,\n",
       " 'total': 1798,\n",
       " 'HasAns_exact': 32.758620689655174,\n",
       " 'HasAns_f1': 56.99536137666129,\n",
       " 'HasAns_total': 1798,\n",
       " 'best_exact': 32.758620689655174,\n",
       " 'best_exact_thresh': 0.0,\n",
       " 'best_f1': 56.99536137666129,\n",
       " 'best_f1_thresh': 0.0}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{'exact': 32.758620689655174, 'f1': 56.99536137666129, 'total': 1798, 'HasAns_exact': 32.758620689655174, 'HasAns_f1': 56.99536137666129, 'HasAns_total': 1798, 'best_exact': 32.758620689655174, 'best_exact_thresh': 0.0, 'best_f1': 56.99536137666129, 'best_f1_thresh': 0.0}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## evaluation on squad 2:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Qualitative evaluation on covid-19 questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODELS_DIRECTORY= \"/data/home/xcs224u_student/notebooks/cs224u/models\"\n",
    "FINAL_SCIBERT_COVID_MODEL = MODELS_DIRECTORY + \"/covidbioasq\"\n",
    "SCIBERT_BIOASQ_MODEL = MODELS_DIRECTORY + \"/Scibertbioasq\"\n",
    "BIOBERT_BIOASQ_MODEL = MODELS_DIRECTORY + \"/biobertbioasq\"\n",
    "qa_scibert_covid_final_pipeline = pipeline('question-answering', model=FINAL_SCIBERT_COVID_MODEL, tokenizer=FINAL_SCIBERT_COVID_MODEL)\n",
    "qa_scibert_bioasq_pipeline = pipeline('question-answering', model=SCIBERT_BIOASQ_MODEL, tokenizer=SCIBERT_BIOASQ_MODEL)\n",
    "qa_biobert_bioasq_final_pipeline = pipeline('question-answering', model=BIOBERT_BIOASQ_MODEL, tokenizer=BIOBERT_BIOASQ_MODEL)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### question 1: What are COVID-19 symptoms?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"What are COVID 19 symptoms?\"\n",
    "context = \"There have been several reports noting anosmia and ageusia as possible\\\n",
    "symptoms of COVID-19. This is of particular interest in oncology since \\\n",
    "patients receiving some cancer treatments such as chemotherapy or immune\\\n",
    "therapy often experience similar symptoms as side-effects. The purpose of\\\n",
    "this report was to summarise the evidence on the existence of anosmia and\\\n",
    "ageusia an emerging COVID-19 symptoms in order to better inform both\\\n",
    "oncology patients and clinicians. Currently, there is no published\\\n",
    "evidence or case reports noting anosmia or ageusia as symptoms of\\\n",
    "COVID-19. Nevertheless, experts in rhinology have suggested that\\\n",
    "the onset of such symptoms could either act as a trigger for\\\n",
    "testing for the disease where possible, or could be a new\\\n",
    "criterion to self-isolate. Whilst more data is currently\\\n",
    "needed to strengthen our knowledge of the symptoms of\\\n",
    "COVID-19, oncology patients who are concerned about\\\n",
    "anosmia or ageusia in the context of their systemic\\\n",
    "anticancer therapy should contact their acute oncology\\\n",
    "support line for advice\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gold answer: \n",
    "\"There have been several reports noting anosmia and ageusia as possible\\\n",
    "symptoms of COVID-19\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 9.500477770581434e-05,\n",
       " 'start': 525,\n",
       " 'end': 567,\n",
       " 'answer': 'anosmia or ageusia as symptoms ofCOVID-19.'}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa_scibert_covid_final_pipeline(context=context, question=question)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 0.00012106725689413692,\n",
       " 'start': 0,\n",
       " 'end': 58,\n",
       " 'answer': 'There have been several reports noting anosmia and ageusia'}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa_scibert_bioasq_pipeline(context=context, question=question)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 0.00018831344740694034,\n",
       " 'start': 0,\n",
       " 'end': 58,\n",
       " 'answer': 'There have been several reports noting anosmia and ageusia'}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa_biobert_bioasq_final_pipeline(context=context, question=question)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### question 2: What is know about COVID-19 transmission?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"What is the COVID 19 seasonality of transmission?\"\n",
    "context = \"This paper investigates the influence of air temperature and\\\n",
    "relative humidity on the transmission of COVID-19. After estimating the\\\n",
    "serial interval of COVID-19 from 105 hand-collected pairs of the virus\\\n",
    "carrier and the infected, we calculate the daily effective reproductive\\\n",
    "number, R, for each of all 100 Chinese cities with more than 40 cases.\\\n",
    "Using the daily R values from January 21 to 23, 2020 as proxies of\\\n",
    "nonintervened transmission intensity, we find, under a linear regression\\\n",
    "framework, high temperature and high humidity significantly reduce the\\\n",
    "transmission of COVID-19, respectively. One-degree Celsius increase\\\n",
    "in temperature and one percent increase in relative humidity lower R\\\n",
    "by 0.0225 and 0.0158, respectively. This result is consistent with \\\n",
    "the fact that the high temperature and high humidity reduce the\\\n",
    "transmission of influenza and SARS. It indicates that the arrival\\\n",
    "of summer and rainy season in the northern hemisphere can effectively\\\n",
    "reduce the transmission of the COVID-19. We also developed a website\\\n",
    "to provide R of major cities around the world according to their daily\\\n",
    "temperature and relative humidity:\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gold answer: \n",
    "\"high temperature and high humidity significantly reduce the\\\n",
    "transmission of COVID-19\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 0.0005917107388225887,\n",
       " 'start': 85,\n",
       " 'end': 110,\n",
       " 'answer': 'transmission of COVID-19.'}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa_scibert_covid_final_pipeline(context=context, question=question)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 0.00015064815747313238,\n",
       " 'start': 851,\n",
       " 'end': 933,\n",
       " 'answer': 'It indicates that the arrivalof summer and rainy season in the northern hemisphere'}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa_scibert_bioasq_pipeline(context=context, question=question)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 0.0001891297260903299,\n",
       " 'start': 938,\n",
       " 'end': 989,\n",
       " 'answer': 'effectivelyreduce the transmission of the COVID-19.'}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa_biobert_bioasq_final_pipeline(context=context, question=question)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### question 3: What is the incubation time?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"What is the incubation time?\"\n",
    "context = \"Currently, the emergence of a novel human coronavirus, \\\n",
    "SARS-CoV-2, has become a global health concern causing severe respiratory\\\n",
    "tract infections in humans. Human-to-human transmissions have been described with incubation times between 2-10 days, \\\n",
    "facilitating its spread via droplets, contaminated hands or surfaces. We therefore reviewed the literature on all \\\n",
    "available information about the persistence of human and veterinary coronaviruses on inanimate surfaces as well as\\\n",
    "inactivation strategies with biocidal agents used for chemical disinfection, e.g. in healthcare facilities. The \\\n",
    "analysis of 22 studies reveals that human coronaviruses such as Severe Acute Respiratory Syndrome (SARS) coronavirus,\\\n",
    "Middle East Respiratory Syndrome (MERS) coronavirus or endemic human coronaviruses (HCoV) can persist on inanimate\\\n",
    "surfaces like metal, glass or plastic for up to 9 days, but can be efficiently inactivated by surface disinfection\\\n",
    "procedures with 62e71% ethanol, 0.5% hydrogen peroxide or 0.1% sodium hypochlorite within 1 minute. Other biocidal\\\n",
    "agents such as 0.05e0.2% benzalkonium chloride or 0.02% chlorhexidine digluconate are less effective. As no specific\\\n",
    "therapies are available for SARS-CoV-2, early containment and prevention of further spread will be crucial to stop the\\\n",
    "ongoing outbreak and to control this novel infectious thread.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "gold answer: \"transmissions have been described with incubation times between 2-10 days\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 0.005668472819849724,\n",
       " 'start': 171,\n",
       " 'end': 245,\n",
       " 'answer': 'transmissions have been described with incubation times between 2-10 days,'}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa_scibert_covid_final_pipeline(context=context, question=question)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 0.0012835383344930867,\n",
       " 'start': 171,\n",
       " 'end': 245,\n",
       " 'answer': 'transmissions have been described with incubation times between 2-10 days,'}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa_scibert_bioasq_pipeline(context=context, question=question)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 0.0001906975930728895,\n",
       " 'start': 210,\n",
       " 'end': 245,\n",
       " 'answer': 'incubation times between 2-10 days,'}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa_biobert_bioasq_final_pipeline(context=context, question=question)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### question 4: What is the COVID-19 persistence of infectious on surfaces of different materials? \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"What is the COVID 19 persistence of infectious on surfaces of different materials? \"\n",
    "context = \"Currently, the emergence of a novel human coronavirus, \\\n",
    "SARS-CoV-2, has become a global health concern causing severe respiratory\\\n",
    "tract infections in humans. Human-to-human transmissions have been described with incubation times between 2-10 days, \\\n",
    "facilitating its spread via droplets, contaminated hands or surfaces. We therefore reviewed the literature on all \\\n",
    "available information about the persistence of human and veterinary coronaviruses on inanimate surfaces as well as\\\n",
    "inactivation strategies with biocidal agents used for chemical disinfection, e.g. in healthcare facilities. The \\\n",
    "analysis of 22 studies reveals that human coronaviruses such as Severe Acute Respiratory Syndrome (SARS) coronavirus,\\\n",
    "Middle East Respiratory Syndrome (MERS) coronavirus or endemic human coronaviruses (HCoV) can persist on inanimate\\\n",
    "surfaces like metal, glass or plastic for up to 9 days, but can be efficiently inactivated by surface disinfection\\\n",
    "procedures with 62e71% ethanol, 0.5% hydrogen peroxide or 0.1% sodium hypochlorite within 1 minute. Other biocidal\\\n",
    "agents such as 0.05e0.2% benzalkonium chloride or 0.02% chlorhexidine digluconate are less effective. As no specific\\\n",
    "therapies are available for SARS-CoV-2, early containment and prevention of further spread will be crucial to stop the\\\n",
    "ongoing outbreak and to control this novel infectious thread.\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gold answer: \n",
    "\"can persist on inanimate\\\n",
    "surfaces like metal, glass or plastic for up to 9 days, but can be efficiently inactivated by surface disinfection\\\n",
    "procedures with 62e71% ethanol, 0.5% hydrogen peroxide or 0.1% sodium hypochlorite within 1 minute\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 0.00024543143346247853,\n",
       " 'start': 110,\n",
       " 'end': 155,\n",
       " 'answer': 'severe respiratorytract infections in humans.'}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa_scibert_covid_final_pipeline(context=context, question=question)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 0.00015886760486662904,\n",
       " 'start': 1023,\n",
       " 'end': 1030,\n",
       " 'answer': 'minute.'}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa_scibert_bioasq_pipeline(context=context, question=question)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 0.0002471508881617922, 'start': 582, 'end': 585, 'answer': 'The'}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa_biobert_bioasq_final_pipeline(context=context, question=question)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
