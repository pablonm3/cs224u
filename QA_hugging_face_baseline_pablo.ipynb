{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "IPython.notebook.set_autosave_interval(10000)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autosaving every 10 seconds\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline, AutoModel, AutoTokenizer, BertTokenizer, BertForQuestionAnswering\n",
    "import transformers\n",
    "import torch\n",
    "import pandas as pd\n",
    "%autosave 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "DATA_DIRECTORY = \"/home/xcs224u_student/notebooks/cs224u/data\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# baselines models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT model trained on squad 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5912e8d2e1a14c338d592b0c6d2a6b45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=465.0, style=ProgressStyle(description_…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e68ca03b216c4491af1a0d4cbe8f6b25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=231508.0, style=ProgressStyle(descripti…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fed2f895a07448f3ae73103c196e3a45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=112.0, style=ProgressStyle(description_…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "adbb5023bea64a2cb53d1105cf6d4714",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=39.0, style=ProgressStyle(description_w…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12f6eebe97a742caae215ea333c42476",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=437985356.0, style=ProgressStyle(descri…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#This model is BERT base uncased trained on SQuAD v2 \n",
    "# https://huggingface.co/twmkn9/bert-base-uncased-squad2\n",
    "BERT_MODEL = \"twmkn9/bert-base-uncased-squad2\"\n",
    "qa_bert_pipeline = pipeline('question-answering', model=BERT_MODEL, tokenizer=BERT_MODEL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 0.9665267544103031, 'start': 0, 'end': 6, 'answer': 'Normans'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa_bert_pipeline(context=\"Normans\", question=\"Who gave their name to Normandy in the 1000's and 1100's\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 0.9665267544103031, 'start': 0, 'end': 6, 'answer': 'Normans'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa_bert_pipeline(context=\"Normans\", question=\"Who gave their name to Normandy in the 1000's and 1100's\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline model using SciBERT - pretrained with SQUAD V2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7530c1e3b3d14f1d807a27f40c929583",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=465.0, style=ProgressStyle(description_…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cadd6e3397e3432f83402c42c0ad7c01",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=227845.0, style=ProgressStyle(descripti…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0fa6b83874d7441d9a7e1197594f8242",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=112.0, style=ProgressStyle(description_…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6892398d418943969313259705725849",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=23.0, style=ProgressStyle(description_w…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "331a2cb1f3ee4d91a25beca28ad6e5a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=439730252.0, style=ProgressStyle(descri…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# allenai/scibert_scivocab_uncased: scibert model fine-tuned on SQuAD V2 \n",
    "# https://huggingface.co/ktrapeznikov/scibert_scivocab_uncased_squad_v2\n",
    "SCIBERT_MODEL = \"ktrapeznikov/scibert_scivocab_uncased_squad_v2\"\n",
    "qa_scibert_pipeline = pipeline('question-answering', model=SCIBERT_MODEL, tokenizer=SCIBERT_MODEL)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline model using BIOBERT - pretrained with SQUAD V2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0289b26a46cc41b8a67f7157674e868b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=465.0, style=ProgressStyle(description_…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73e723c482554a6f81681bc3003ad2bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=213450.0, style=ProgressStyle(descripti…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "adaa4b93e8364738a1f0888dd4d033f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=112.0, style=ProgressStyle(description_…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9c07eeb2e1a4488a34215c2c091d25d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=136.0, style=ProgressStyle(description_…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e304dae0ae754f1da414e7efeb3ac53b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=433297484.0, style=ProgressStyle(descri…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# biobert model fine-tuned on SQuAD V2 \n",
    "# https://huggingface.co/ktrapeznikov/biobert_v1.1_pubmed_squad_v2\n",
    "BIOBERT_MODEL = \"ktrapeznikov/biobert_v1.1_pubmed_squad_v2\"\n",
    "qa_biobert_pipeline = pipeline('question-answering', model=BIOBERT_MODEL, tokenizer=BIOBERT_MODEL)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Qualitative comparisons on covid-19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONVID_ARTICLE_EASY=\"\"\"Coronavirus disease 2019 (COVID-19) is an infectious disease caused by severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2).\n",
    "The disease was first identified in December 2019 in Wuhan, the capital of China's Hubei province, and has since spread globally, resulting in the ongoing 2019–20 coronavirus pandemic.\n",
    "Common symptoms include fever, cough and shortness of breath.[5] Other symptoms may include fatigue, muscle pain, diarrhoea, sore throat, loss of smell and abdominal pain.[5][11][12] The time from exposure to onset of symptoms is typically around five days, but may range from two to fourteen days.[5][13] While the majority of cases result in mild symptoms, some progress to viral pneumonia and multi-organ failure.\n",
    "As of 10 April 2020, more than 1.67 million[7] cases have been reported in more than 200 countries and territories,[15] resulting in more than 101,000 deaths.[7] More than 372,000 people have recovered. The virus is mainly spread between people during close contact,[a] often via small droplets produced during coughing,[b] sneezing, or talking.\n",
    "While these droplets are produced when breathing out,\n",
    "they usually fall to the ground or surfaces rather than being infectious over large distances.[6][19][20] People may also become infected by touching a contaminated surface and then their face.\n",
    "The virus can survive on surfaces for up to 72 hours.\n",
    "Coronavirus is most contagious during the first three days after onset of symptoms, although spread may be possible before symptoms\n",
    "appear and in later stages of the disease.The standard method of diagnosis is by real-time reverse transcription polymerase chain reaction (rRT-PCR) from a nasopharyngeal swab.[23] The infection can also be diagnosed from a combination of symptoms, risk factors and a chest CT scan showing features of pneumonia.Recommended measures to prevent infection include frequent hand washing, maintaining physical distance from others (especially from those with symptoms), covering coughs and sneezes with a tissue or inner elbow and keeping unwashed hands away from the face.\n",
    "The use of masks is recommended for those who suspect they have the virus and their caregivers.[28] Recommendations for mask use by the general public vary, with some authorities recommending against their use, some recommending their use and others requiring their use.\n",
    "Currently, there is no vaccine or specific antiviral treatment for COVID-19.\n",
    "Management involves treatment of symptoms, supportive care, isolation and experimental measures.The World Health Organization (WHO) declared the 2019–20 coronavirus outbreak a Public Health Emergency of International Concern (PHEIC)[33][34] on 30 January 2020 and a pandemic on 11 March 2020.[10] Local transmission of the disease has been recorded in many countries across all six WHO regions.[35]\"\"\"\n",
    "CONVID_ARTICLE_MEDIUM1=\"\"\"A familial cluster of 5 patients with COVID-19\n",
    "pneumonia in Anyang, China, had contact before their symptom onset with an asymptomatic family member who had\n",
    "traveled from the epidemic center of Wuhan. The sequence\n",
    "of events suggests that the coronavirus may have been transmitted by the asymptomatic carrier. The incubation period\n",
    "for patient 1 was 19 days, which is long but within the\n",
    "reported range of 0 to 24 days.4 Her first RT-PCR result was\n",
    "negative; false-negative results have been observed related to\n",
    "the quality of the kit, the collected sample, or performance of\n",
    "the test. RT-PCR has been widely deployed in diagnostic\n",
    "virology and has yielded few false-positive outcomes.5 Thus,\n",
    "her second RT-PCR result was unlikely to have been a falsepositive and was used to define infection with the coronavirus that causes COVID-19.\n",
    "One previous study reported an asymptomatic 10-yearold boy with COVID-19 infection, but he had abnormalities\n",
    "on chest CT.6 If the findings in this report of presumed transmission by an asymptomatic carrier are replicated, the prevention of COVID-19 infection would prove challenging.\n",
    "The mechanism by which asymptomatic carriers could acquire and transmit the coronavirus that causes COVID-19\n",
    "requires further study\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT SQUAD2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 0.5013612845677784,\n",
       " 'start': 342,\n",
       " 'end': 382,\n",
       " 'answer': 'fever, cough and shortness of breath.[5]'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa_bert_pipeline(context=CONVID_ARTICLE_EASY, question='What are the symptoms')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 0.09125602338691152,\n",
       " 'start': 644,\n",
       " 'end': 685,\n",
       " 'answer': 'has yielded few false-positive outcomes.5'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa_bert_pipeline(context=CONVID_ARTICLE_MEDIUM1, question='does RT-PCR work')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SCIBERT SQUAD2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 0.19227549805068556,\n",
       " 'start': 318,\n",
       " 'end': 382,\n",
       " 'answer': 'Common symptoms include fever, cough and shortness of breath.[5]'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa_scibert_pipeline(context=CONVID_ARTICLE_EASY, question='What are the symptoms')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 0.1103179816038562,\n",
       " 'start': 644,\n",
       " 'end': 685,\n",
       " 'answer': 'has yielded few false-positive outcomes.5'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa_scibert_pipeline(context=CONVID_ARTICLE_MEDIUM1, question='does RT-PCR work')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BIOBERT SQUAD2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 0.4340334721582906,\n",
       " 'start': 342,\n",
       " 'end': 382,\n",
       " 'answer': 'fever, cough and shortness of breath.[5]'}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa_biobert_pipeline(context=CONVID_ARTICLE_EASY, question='What are the symptoms')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 0.09305679627574825,\n",
       " 'start': 585,\n",
       " 'end': 639,\n",
       " 'answer': 'RT-PCR has been widely deployed in diagnostic virology'}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa_biobert_pipeline(context=CONVID_ARTICLE_MEDIUM1, question='does RT-PCR work')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# baseline metrics on SQUAD2:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT base uncased SQUAD2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### command:\n",
    "\n",
    "\n",
    "python run_squad.py \\\n",
    "  --model_type bert \\\n",
    "  --model_name_or_path twmkn9/bert-base-uncased-squad2 \\\n",
    "  --version_2_with_negative \\\n",
    "  --do_lower_case \\\n",
    "  --do_eval \\\n",
    "  --predict_file  /data/home/xcs224u_student/notebooks/cs224u/data/squad/dev-v2.0.json\\\n",
    "  --per_gpu_eval_batch_size 12 \\\n",
    "  --max_answer_length 128 \\\n",
    "  --max_seq_length 384 \\\n",
    "  --doc_stride 128 \\\n",
    "  --overwrite_cache \\\n",
    "  --output_dir /data/home/xcs224u_student/notebooks/cs224u/data/squad/pred_outputs/bert"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "{'exact': 72.21426766613324, 'f1': 75.74594024678946, 'total': 11873, 'HasAns_exact': 72.55398110661268, 'HasAns_f1': 79.62745420886144, 'HasAns_total': 5928, 'NoAns_exact': 71.87552565180825, 'NoAns_f1': 71.87552565180825, 'NoAns_total': 5945, 'best_exact': 72.21426766613324, 'best_exact_thresh': 0.0, 'best_f1': 75.74594024678946, 'best_f1_thresh': 0.0}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## biobert SQUAD2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### command: \n",
    "\n",
    "python run_squad.py \\\n",
    "  --model_type bert \\\n",
    "  --model_name_or_path ktrapeznikov/biobert_v1.1_pubmed_squad_v2 \\\n",
    "  --version_2_with_negative \\\n",
    "  --do_eval \\\n",
    "  --predict_file  /data/home/xcs224u_student/notebooks/cs224u/data/squad/dev-v2.0.json\\\n",
    "  --per_gpu_eval_batch_size 12 \\\n",
    "  --max_answer_length 128 \\\n",
    "  --max_seq_length 384 \\\n",
    "  --doc_stride 128 \\\n",
    "  --overwrite_cache \\\n",
    "  --output_dir /data/home/xcs224u_student/notebooks/cs224u/data/squad/pred_outputs/biobert\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'exact': 75.92857744462225,\n",
       " 'f1': 79.33147378635734,\n",
       " 'total': 11873,\n",
       " 'HasAns_exact': 74.07219973009447,\n",
       " 'HasAns_f1': 80.88775105692005,\n",
       " 'HasAns_total': 5928,\n",
       " 'NoAns_exact': 77.77964676198486,\n",
       " 'NoAns_f1': 77.77964676198486,\n",
       " 'NoAns_total': 5945,\n",
       " 'best_exact': 75.92857744462225,\n",
       " 'best_exact_thresh': 0.0,\n",
       " 'best_f1': 79.33147378635738,\n",
       " 'best_f1_thresh': 0.0}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{'exact': 75.92857744462225, 'f1': 79.33147378635734, 'total': 11873, 'HasAns_exact': 74.07219973009447, 'HasAns_f1': 80.88775105692005, 'HasAns_total': 5928, 'NoAns_exact': 77.77964676198486, 'NoAns_f1': 77.77964676198486, 'NoAns_total': 5945, 'best_exact': 75.92857744462225, 'best_exact_thresh': 0.0, 'best_f1': 79.33147378635738, 'best_f1_thresh': 0.0}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## scibert SQUAD2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### command:\n",
    "python run_squad.py \\\n",
    "  --model_type bert \\\n",
    "  --model_name_or_path ktrapeznikov/scibert_scivocab_uncased_squad_v2 \\\n",
    "  --version_2_with_negative \\\n",
    "  --do_lower_case \\\n",
    "  --do_eval \\\n",
    "  --predict_file  /data/home/xcs224u_student/notebooks/cs224u/data/squad/dev-v2.0.json\\\n",
    "  --per_gpu_eval_batch_size 12 \\\n",
    "  --max_answer_length 128 \\\n",
    "  --max_seq_length 384 \\\n",
    "  --doc_stride 128 \\\n",
    "  --overwrite_cache \\\n",
    "  --output_dir /data/home/xcs224u_student/notebooks/cs224u/data/squad/pred_outputs/scibert\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'exact': 74.98526067548218,\n",
       " 'f1': 78.44016223271369,\n",
       " 'total': 11873,\n",
       " 'HasAns_exact': 70.58029689608637,\n",
       " 'HasAns_f1': 77.5000077916683,\n",
       " 'HasAns_total': 5928,\n",
       " 'NoAns_exact': 79.37762825904122,\n",
       " 'NoAns_f1': 79.37762825904122,\n",
       " 'NoAns_total': 5945,\n",
       " 'best_exact': 74.99368314663522,\n",
       " 'best_exact_thresh': 0.0,\n",
       " 'best_f1': 78.44858470386681,\n",
       " 'best_f1_thresh': 0.0}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{'exact': 74.98526067548218, 'f1': 78.44016223271369, 'total': 11873, 'HasAns_exact': 70.58029689608637, 'HasAns_f1': 77.5000077916683, 'HasAns_total': 5928, 'NoAns_exact': 79.37762825904122, 'NoAns_f1': 79.37762825904122, 'NoAns_total': 5945, 'best_exact': 74.99368314663522, 'best_exact_thresh': 0.0, 'best_f1': 78.44858470386681, 'best_f1_thresh': 0.0}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# baseline metrics on BIOASQ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT base uncased SQUAD2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### command: \n",
    "\n",
    "````\n",
    "export TEST_SET_NAME=BioASQ-task8bPhaseB-testset_combined_squad_format.json\n",
    "export DATA_DIRECTORY=/data/home/xcs224u_student/notebooks/cs224u/data/bioasq\n",
    "python run_squad.py \\\n",
    "  --model_type bert \\\n",
    "  --model_name_or_path twmkn9/bert-base-uncased-squad2 \\\n",
    "  --version_2_with_negative \\\n",
    "  --do_lower_case \\\n",
    "  --do_eval \\\n",
    "  --predict_file $DATA_DIRECTORY/BioASQ-test8b/$TEST_SET_NAME\\\n",
    "  --per_gpu_eval_batch_size 12 \\\n",
    "  --max_answer_length 128 \\\n",
    "  --max_seq_length 384 \\\n",
    "  --doc_stride 128 \\\n",
    "  --overwrite_cache \\\n",
    "  --output_dir $DATA_DIRECTORY/pred_outputs/bert \n",
    "  ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "{'exact': 0.7230255839822024, 'f1': 16.179300126709233, 'total': 1798, 'HasAns_exact': 0.7230255839822024, 'HasAns_f1': 16.179300126709233, 'HasAns_total': 1798, 'best_exact': 0.7230255839822024, 'best_exact_thresh': 0.0, 'best_f1': 16.179300126709233, 'best_f1_thresh': 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## biobert SQUAD2(this model is cased, so don't lowercase)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "export TEST_SET_NAME=BioASQ-task8bPhaseB-testset_combined_squad_format.json\n",
    "export DATA_DIRECTORY=/data/home/xcs224u_student/notebooks/cs224u/data/bioasq\n",
    "python run_squad.py \\\n",
    "  --model_type bert \\\n",
    "  --model_name_or_path ktrapeznikov/biobert_v1.1_pubmed_squad_v2 \\\n",
    "  --version_2_with_negative \\\n",
    "  --do_eval \\\n",
    "  --predict_file $DATA_DIRECTORY/BioASQ-test8b/$TEST_SET_NAME\\\n",
    "  --per_gpu_eval_batch_size 12 \\\n",
    "  --max_answer_length 128 \\\n",
    "  --max_seq_length 384 \\\n",
    "  --doc_stride 128 \\\n",
    "  --overwrite_cache \\\n",
    "  --output_dir $DATA_DIRECTORY/pred_outputs/biobert "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### results:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "{'exact': 0.8342602892102335, 'f1': 16.69560652289738, 'total': 1798, 'HasAns_exact': 0.8342602892102335, 'HasAns_f1': 16.69560652289738, 'HasAns_total': 1798, 'best_exact': 0.8342602892102335, 'best_exact_thresh': 0.0, 'best_f1': 16.69560652289738, 'best_f1_thresh': 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## scibert finedtuned on squad2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "export TEST_SET_NAME=BioASQ-task8bPhaseB-testset_combined_squad_format.json\n",
    "export DATA_DIRECTORY=/data/home/xcs224u_student/notebooks/cs224u/data/bioasq\n",
    "python run_squad.py \\\n",
    "  --model_type bert \\\n",
    "  --model_name_or_path ktrapeznikov/scibert_scivocab_uncased_squad_v2 \\\n",
    "   --version_2_with_negative \\\n",
    "  --do_lower_case \\\n",
    "  --do_eval \\\n",
    "  --predict_file $DATA_DIRECTORY/BioASQ-test8b/$TEST_SET_NAME\\\n",
    "  --per_gpu_eval_batch_size 12 \\\n",
    "  --max_answer_length 128 \\\n",
    "  --max_seq_length 384 \\\n",
    "  --doc_stride 128 \\\n",
    "  --overwrite_cache \\\n",
    "  --output_dir $DATA_DIRECTORY/pred_outputs/scibert \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'exact': 1.0567296996662958,\n",
       " 'f1': 18.87351618082947,\n",
       " 'total': 1798,\n",
       " 'HasAns_exact': 1.0567296996662958,\n",
       " 'HasAns_f1': 18.87351618082947,\n",
       " 'HasAns_total': 1798,\n",
       " 'best_exact': 1.0567296996662958,\n",
       " 'best_exact_thresh': 0.0,\n",
       " 'best_f1': 18.87351618082947,\n",
       " 'best_f1_thresh': 0.0}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{'exact': 1.0567296996662958, 'f1': 18.87351618082947, 'total': 1798, 'HasAns_exact': 1.0567296996662958, 'HasAns_f1': 18.87351618082947, 'HasAns_total': 1798, 'best_exact': 1.0567296996662958, 'best_exact_thresh': 0.0, 'best_f1': 18.87351618082947, 'best_f1_thresh': 0.0}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finetuning baseline models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "command:\n",
    "\n",
    "```\n",
    "DATA_DIR=/home/xcs224u_student/notebooks/cs224u/data/bioasq\n",
    "OUTPUT_MODEL=/data/home/xcs224u_student/notebooks/cs224u/models/bert_bioasq_try2\n",
    "BASE_MODEL=twmkn9/bert-base-uncased-squad2\n",
    "python run_squad.py \\\n",
    "  --version_2_with_negative \\\n",
    "  --model_type bert \\\n",
    "  --model_name_or_path $BASE_MODEL \\\n",
    "  --output_dir $OUTPUT_MODEL \\\n",
    "  --do_eval \\\n",
    "  --do_lower_case \\\n",
    "  --train_file $DATA_DIR/BioASQ-training8b/training8b_squad_format_3243.json \\\n",
    "  --predict_file $DATA_DIR/BioASQ-test8b/BioASQ-task8bPhaseB-testset_combined_squad_format.json \\\n",
    "  --per_gpu_train_batch_size 12 \\\n",
    "  --per_gpu_eval_batch_size 12 \\\n",
    "  --learning_rate 3e-5 \\\n",
    "  --num_train_epochs 3.0 \\\n",
    "  --max_seq_length 384 \\\n",
    "  --max_answer_length 128 \\\n",
    "  --doc_stride 128 \\\n",
    "  --save_steps 2000 \\\n",
    "  --threads 24 \\\n",
    "  --warmup_steps 550 \\\n",
    "  --gradient_accumulation_steps 1 \\\n",
    "  --fp16 \\\n",
    "  --logging_steps 50 \\\n",
    "  --do_train\n",
    "\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### metrics evaluated on bioasq:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'exact': 6.722689075630252,\n",
       " 'f1': 27.284190864183085,\n",
       " 'total': 119,\n",
       " 'HasAns_exact': 6.722689075630252,\n",
       " 'HasAns_f1': 27.284190864183085,\n",
       " 'HasAns_total': 119,\n",
       " 'best_exact': 6.722689075630252,\n",
       " 'best_exact_thresh': 0.0,\n",
       " 'best_f1': 27.284190864183085,\n",
       " 'best_f1_thresh': 0.0}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{'exact': 6.722689075630252, 'f1': 27.284190864183085, 'total': 119, 'HasAns_exact': 6.722689075630252, 'HasAns_f1': 27.284190864183085, 'HasAns_total': 119, 'best_exact': 6.722689075630252, 'best_exact_thresh': 0.0, 'best_f1': 27.284190864183085, 'best_f1_thresh': 0.0}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### metrics evaluated on squad2:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### command:\n",
    "\n",
    "\n",
    "```\n",
    "python run_squad.py \\\n",
    "  --model_type bert \\\n",
    "  --model_name_or_path /data/home/xcs224u_student/notebooks/cs224u/models/bert_bioasq_try2 \\\n",
    "   --version_2_with_negative \\\n",
    "  --do_lower_case \\\n",
    "  --do_eval \\\n",
    "  --predict_file  /data/home/xcs224u_student/notebooks/cs224u/data/squad/dev-v2.0.json\\\n",
    "  --max_seq_length 384 \\\n",
    "  --doc_stride 128 \\\n",
    "  --overwrite_cache \\\n",
    "  --max_answer_length 128 \\\n",
    "  --per_gpu_eval_batch_size 12 \\\n",
    "  --output_dir /data/home/xcs224u_student/notebooks/cs224u/data/squad/pred_outputs/bert\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'exact': 30.589543937708566,\n",
       " 'f1': 54.9644942765143,\n",
       " 'total': 1798,\n",
       " 'HasAns_exact': 30.589543937708566,\n",
       " 'HasAns_f1': 54.9644942765143,\n",
       " 'HasAns_total': 1798,\n",
       " 'best_exact': 30.589543937708566,\n",
       " 'best_exact_thresh': 0.0,\n",
       " 'best_f1': 54.9644942765143,\n",
       " 'best_f1_thresh': 0.0}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{'exact': 30.589543937708566, 'f1': 54.9644942765143, 'total': 1798, 'HasAns_exact': 30.589543937708566, 'HasAns_f1': 54.9644942765143, 'HasAns_total': 1798, 'best_exact': 30.589543937708566, 'best_exact_thresh': 0.0, 'best_f1': 54.9644942765143, 'best_f1_thresh': 0.0}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BIOBERT(remember its cased)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "DATA_DIR=/home/xcs224u_student/notebooks/cs224u/data/bioasq\n",
    "OUTPUT_MODEL=/data/home/xcs224u_student/notebooks/cs224u/models/biobert_bioasq_try2\n",
    "BASE_MODEL=ktrapeznikov/biobert_v1.1_pubmed_squad_v2\n",
    "python run_squad.py \\\n",
    "  --version_2_with_negative \\\n",
    "  --model_type bert \\\n",
    "  --model_name_or_path $BASE_MODEL \\\n",
    "  --output_dir $OUTPUT_MODEL \\\n",
    "  --do_eval \\\n",
    "  --train_file $DATA_DIR/BioASQ-training8b/training8b_squad_format_3243.json \\\n",
    "  --predict_file $DATA_DIR/BioASQ-test8b/BioASQ-task8bPhaseB-testset_combined_squad_format.json \\\n",
    "  --per_gpu_train_batch_size 12 \\\n",
    "  --per_gpu_eval_batch_size 12 \\\n",
    "  --learning_rate 3e-5 \\\n",
    "  --num_train_epochs 3.0 \\\n",
    "  --max_seq_length 384 \\\n",
    "  --gradient_accumulation_steps 1 \\\n",
    "  --doc_stride 128 \\\n",
    "  --max_answer_length 128 \\\n",
    "  --save_steps 2000 \\\n",
    "  --threads 24 \\\n",
    "  --warmup_steps 550 \\\n",
    "  --gradient_accumulation_steps 1 \\\n",
    "  --fp16 \\\n",
    "  --logging_steps 50 \\\n",
    "  --do_train\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### metrics evaluated on bioasq:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'exact': 33.42602892102336,\n",
       " 'f1': 57.66916736505339,\n",
       " 'total': 1798,\n",
       " 'HasAns_exact': 33.42602892102336,\n",
       " 'HasAns_f1': 57.66916736505339,\n",
       " 'HasAns_total': 1798,\n",
       " 'best_exact': 33.42602892102336,\n",
       " 'best_exact_thresh': 0.0,\n",
       " 'best_f1': 57.66916736505339,\n",
       " 'best_f1_thresh': 0.0}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{'exact': 33.42602892102336, 'f1': 57.66916736505339, 'total': 1798, 'HasAns_exact': 33.42602892102336, 'HasAns_f1': 57.66916736505339, 'HasAns_total': 1798, 'best_exact': 33.42602892102336, 'best_exact_thresh': 0.0, 'best_f1': 57.66916736505339, 'best_f1_thresh': 0.0}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### metrics evaluated on squad2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### command:\n",
    "\n",
    "\n",
    "```\n",
    "python run_squad.py \\\n",
    "  --model_type bert \\\n",
    "  --model_name_or_path /data/home/xcs224u_student/notebooks/cs224u/models/biobert_bioasq_try2 \\\n",
    "  --version_2_with_negative \\\n",
    "  --do_eval \\\n",
    "  --predict_file  /data/home/xcs224u_student/notebooks/cs224u/data/squad/dev-v2.0.json\\\n",
    "  --max_seq_length 384 \\\n",
    "  --doc_stride 128 \\\n",
    "  --overwrite_cache \\\n",
    "  --max_answer_length 128 \\\n",
    "  --per_gpu_eval_batch_size 12 \\\n",
    "  --output_dir /data/home/xcs224u_student/notebooks/cs224u/data/squad/pred_outputs/biobert_bioasq\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'exact': 18.75684325781184,\n",
       " 'f1': 31.365497603748768,\n",
       " 'total': 11873,\n",
       " 'HasAns_exact': 6.325910931174089,\n",
       " 'HasAns_f1': 31.579378044755565,\n",
       " 'HasAns_total': 5928,\n",
       " 'NoAns_exact': 31.152228763666947,\n",
       " 'NoAns_f1': 31.152228763666947,\n",
       " 'NoAns_total': 5945,\n",
       " 'best_exact': 50.08843594710688,\n",
       " 'best_exact_thresh': 0.0,\n",
       " 'best_f1': 50.09557499408422,\n",
       " 'best_f1_thresh': 0.0}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{'exact': 18.75684325781184, 'f1': 31.365497603748768, 'total': 11873, 'HasAns_exact': 6.325910931174089, 'HasAns_f1': 31.579378044755565, 'HasAns_total': 5928, 'NoAns_exact': 31.152228763666947, 'NoAns_f1': 31.152228763666947, 'NoAns_total': 5945, 'best_exact': 50.08843594710688, 'best_exact_thresh': 0.0, 'best_f1': 50.09557499408422, 'best_f1_thresh': 0.0}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SCIBERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "DATA_DIR=/home/xcs224u_student/notebooks/cs224u/data/bioasq\n",
    "OUTPUT_MODEL=/data/home/xcs224u_student/notebooks/cs224u/models/scibert_bioasq_try5\n",
    "BASE_MODEL=ktrapeznikov/scibert_scivocab_uncased_squad_v2\n",
    "python run_squad.py \\\n",
    "  --version_2_with_negative \\\n",
    "  --model_type bert \\\n",
    "  --model_name_or_path $BASE_MODEL \\\n",
    "  --output_dir $OUTPUT_MODEL \\\n",
    "  --do_eval \\\n",
    "  --do_lower_case \\\n",
    "  --train_file $DATA_DIR/BioASQ-training8b/training8b_squad_format_3243.json \\\n",
    "  --predict_file $DATA_DIR/BioASQ-test8b/BioASQ-task8bPhaseB-testset_combined_squad_format.json \\\n",
    "  --per_gpu_train_batch_size 12 \\\n",
    "  --per_gpu_eval_batch_size 12 \\\n",
    "  --learning_rate 3e-5 \\\n",
    "  --num_train_epochs 3.0 \\\n",
    "  --max_seq_length 384 \\\n",
    "  --gradient_accumulation_steps 1 \\\n",
    "  --max_answer_length 128 \\\n",
    "  --doc_stride 128 \\\n",
    "  --save_steps 2000 \\\n",
    "  --threads 24 \\\n",
    "  --warmup_steps 550 \\\n",
    "  --gradient_accumulation_steps 1 \\\n",
    "  --fp16 \\\n",
    "  --logging_steps 50 \\\n",
    "  --do_train\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### metrics evaluated on bioasq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'exact': 35.81757508342603,\n",
       " 'f1': 60.45343274509798,\n",
       " 'total': 1798,\n",
       " 'HasAns_exact': 35.81757508342603,\n",
       " 'HasAns_f1': 60.45343274509798,\n",
       " 'HasAns_total': 1798,\n",
       " 'best_exact': 35.81757508342603,\n",
       " 'best_exact_thresh': 0.0,\n",
       " 'best_f1': 60.45343274509798,\n",
       " 'best_f1_thresh': 0.0}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{'exact': 35.81757508342603, 'f1': 60.45343274509798, 'total': 1798, 'HasAns_exact': 35.81757508342603, 'HasAns_f1': 60.45343274509798, 'HasAns_total': 1798, 'best_exact': 35.81757508342603, 'best_exact_thresh': 0.0, 'best_f1': 60.45343274509798, 'best_f1_thresh': 0.0}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### metrics evaluated on squad 2:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### command:\n",
    "\n",
    "\n",
    "```\n",
    "python run_squad.py \\\n",
    "  --model_type bert \\\n",
    "  --model_name_or_path /data/home/xcs224u_student/notebooks/cs224u/models/scibert_bioasq_try5 \\\n",
    "  --version_2_with_negative \\\n",
    "  --do_lower_case \\\n",
    "  --do_eval \\\n",
    "  --predict_file  /data/home/xcs224u_student/notebooks/cs224u/data/squad/dev-v2.0.json\\\n",
    "  --max_seq_length 384 \\\n",
    "  --max_answer_length 128 \\\n",
    "  --doc_stride 128 \\\n",
    "  --per_gpu_eval_batch_size 12 \\\n",
    "  --threads 24 \\\n",
    "  --overwrite_cache \\\n",
    "  --output_dir /data/home/xcs224u_student/notebooks/cs224u/data/squad/pred_outputs/scibert_bioasq_try5\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'exact': 20.508717257643394,\n",
       " 'f1': 32.61155153224052,\n",
       " 'total': 11873,\n",
       " 'HasAns_exact': 3.9642375168690958,\n",
       " 'HasAns_f1': 28.204613924138688,\n",
       " 'HasAns_total': 5928,\n",
       " 'NoAns_exact': 37.00588730025231,\n",
       " 'NoAns_f1': 37.00588730025231,\n",
       " 'NoAns_total': 5945,\n",
       " 'best_exact': 50.07159100480081,\n",
       " 'best_exact_thresh': 0.0,\n",
       " 'best_f1': 50.081828652708694,\n",
       " 'best_f1_thresh': 0.0}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{'exact': 20.508717257643394, 'f1': 32.61155153224052, 'total': 11873, 'HasAns_exact': 3.9642375168690958, 'HasAns_f1': 28.204613924138688, 'HasAns_total': 5928, 'NoAns_exact': 37.00588730025231, 'NoAns_f1': 37.00588730025231, 'NoAns_total': 5945, 'best_exact': 50.07159100480081, 'best_exact_thresh': 0.0, 'best_f1': 50.081828652708694, 'best_f1_thresh': 0.0}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finetune scibert-covid-squad on bioasq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## smoke test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "SCIBERT_COVID_SQUAD_MODEL = \"/data/home/xcs224u_student/notebooks/cs224u/models/scibert_covid_squad\"\n",
    "qa_scibert_covid_squad_pipeline = pipeline('question-answering', model=SCIBERT_COVID_SQUAD_MODEL, tokenizer=SCIBERT_COVID_SQUAD_MODEL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 0.31775387244431386,\n",
       " 'start': 318,\n",
       " 'end': 382,\n",
       " 'answer': 'Common symptoms include fever, cough and shortness of breath.[5]'}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa_scibert_covid_squad_pipeline(context=CONVID_ARTICLE_EASY, question='What are the symptoms')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## fine tuning on bioasq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### command: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "DATA_DIR=/home/xcs224u_student/notebooks/cs224u/data/bioasq\n",
    "OUTPUT_MODEL=/data/home/xcs224u_student/notebooks/cs224u/models/scibert_covid_squad_bioasq\n",
    "BASE_MODEL=/data/home/xcs224u_student/notebooks/cs224u/models/scibert_covid_squad\n",
    "python run_squad.py \\\n",
    "  --version_2_with_negative \\\n",
    "  --model_type bert \\\n",
    "  --model_name_or_path $BASE_MODEL \\\n",
    "  --output_dir $OUTPUT_MODEL \\\n",
    "  --do_eval \\\n",
    "  --do_lower_case \\\n",
    "  --train_file $DATA_DIR/BioASQ-training8b/training8b_squad_format_3243.json \\\n",
    "  --predict_file $DATA_DIR/BioASQ-test8b/BioASQ-task8bPhaseB-testset_combined_squad_format.json \\\n",
    "  --per_gpu_train_batch_size 12 \\\n",
    "  --per_gpu_eval_batch_size 12 \\\n",
    "  --learning_rate 3e-5 \\\n",
    "  --num_train_epochs 3.0 \\\n",
    "  --max_seq_length 384 \\\n",
    "  --gradient_accumulation_steps 1 \\\n",
    "  --max_answer_length 128 \\\n",
    "  --doc_stride 128 \\\n",
    "  --save_steps 2000 \\\n",
    "  --threads 24 \\\n",
    "  --warmup_steps 550 \\\n",
    "  --gradient_accumulation_steps 1 \\\n",
    "  --fp16 \\\n",
    "  --logging_steps 50 \\\n",
    "  --do_train\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## evaluation on bioasq:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'exact': 32.758620689655174,\n",
       " 'f1': 56.99536137666129,\n",
       " 'total': 1798,\n",
       " 'HasAns_exact': 32.758620689655174,\n",
       " 'HasAns_f1': 56.99536137666129,\n",
       " 'HasAns_total': 1798,\n",
       " 'best_exact': 32.758620689655174,\n",
       " 'best_exact_thresh': 0.0,\n",
       " 'best_f1': 56.99536137666129,\n",
       " 'best_f1_thresh': 0.0}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{'exact': 32.758620689655174, 'f1': 56.99536137666129, 'total': 1798, 'HasAns_exact': 32.758620689655174, 'HasAns_f1': 56.99536137666129, 'HasAns_total': 1798, 'best_exact': 32.758620689655174, 'best_exact_thresh': 0.0, 'best_f1': 56.99536137666129, 'best_f1_thresh': 0.0}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## evaluation on squad 2:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### command:\n",
    "\n",
    "\n",
    "```\n",
    "python run_squad.py \\\n",
    "  --model_type bert \\\n",
    "  --model_name_or_path /data/home/xcs224u_student/notebooks/cs224u/models/scibert_covid_squad_bioasq \\\n",
    "  --version_2_with_negative \\\n",
    "  --do_lower_case \\\n",
    "  --do_eval \\\n",
    "  --predict_file  /data/home/xcs224u_student/notebooks/cs224u/data/squad/dev-v2.0.json\\\n",
    "  --max_seq_length 384 \\\n",
    "  --doc_stride 128 \\\n",
    "  --overwrite_cache \\\n",
    "  --max_answer_length 128 \\\n",
    "  --per_gpu_eval_batch_size 12 \\\n",
    "  --output_dir /data/home/xcs224u_student/notebooks/cs224u/data/squad/pred_outputs/bert\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'exact': 24.509391055335634,\n",
       " 'f1': 35.39656966563804,\n",
       " 'total': 11873,\n",
       " 'HasAns_exact': 13.49527665317139,\n",
       " 'HasAns_f1': 35.3008555398319,\n",
       " 'HasAns_total': 5928,\n",
       " 'NoAns_exact': 35.49201009251472,\n",
       " 'NoAns_f1': 35.49201009251472,\n",
       " 'NoAns_total': 5945,\n",
       " 'best_exact': 50.08001347595385,\n",
       " 'best_exact_thresh': 0.0,\n",
       " 'best_f1': 50.09126307029111,\n",
       " 'best_f1_thresh': 0.0}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{'exact': 24.509391055335634, 'f1': 35.39656966563804, 'total': 11873, 'HasAns_exact': 13.49527665317139, 'HasAns_f1': 35.3008555398319, 'HasAns_total': 5928, 'NoAns_exact': 35.49201009251472, 'NoAns_f1': 35.49201009251472, 'NoAns_total': 5945, 'best_exact': 50.08001347595385, 'best_exact_thresh': 0.0, 'best_f1': 50.09126307029111, 'best_f1_thresh': 0.0}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Qualitative evaluation on covid-19 questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
